{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Voynich.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSw7umh-QL9h"
      },
      "source": [
        "# Setup Work Environment\n",
        "\n",
        "Run the following cell to setup all the dependencies and the code. There should be no changes required from this cell.\n",
        "\n",
        "Don't forget to ensure the GPU has already attached to your working environment. You can check it in `Runtime -> Manage sessions -> Check if word GPU is available next to the notebook's name`, you can also double check in `Runtime -> Change runtime type -> Check if GPU has already selected from the dropdown menu`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XmdHh775Jsh",
        "outputId": "ee20f51c-5d0a-410b-bd38-cfa16bbe8827"
      },
      "source": [
        "!rm -rf xib\n",
        "!pip install pytrie enlighten colorlog inflection ipapy\n",
        "!git clone https://github.com/akurniawan/xib.git\n",
        "!cd xib && git clone https://github.com/j-luo93/dev_misc.git && cd dev_misc && git checkout b44fde842a6311e03f731cd4e110dcd9fc394db7 && pip install -e .\n",
        "!cd xib && pip install -e ."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytrie\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/19/15ec77ab9c85f7c36eb590d6ab7dd529f8c8516c0e2219f1a77a99d7ee77/PyTrie-0.4.0.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 2.5MB/s \n",
            "\u001b[?25hCollecting enlighten\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/15/7a22630323eb816bd560bb2b60b98c9c829a3fb90f55d9d224f3aa4d7bf3/enlighten-1.10.1-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.8MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/32/e6/e9ddc6fa1104fda718338b341e4b3dc31cd8039ab29e52fc73b508515361/colorlog-5.0.1-py2.py3-none-any.whl\n",
            "Collecting inflection\n",
            "  Downloading https://files.pythonhosted.org/packages/59/91/aa6bde563e0085a02a435aa99b49ef75b0a4b062635e606dab23ce18d720/inflection-0.5.1-py2.py3-none-any.whl\n",
            "Collecting ipapy\n",
            "  Downloading https://files.pythonhosted.org/packages/41/0d/7e8652df6af20a61bb3315f5c9d99fb9ea8f3779ff80fca9d71001230f90/ipapy-0.0.9.0.tar.gz\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pytrie) (2.4.0)\n",
            "Collecting blessed>=1.17.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/35/a781470488a304f66843d328052b6cb22df7163246fb47a27bfb21fba4e6/blessed-1.18.0-py2.py3-none-any.whl (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.5MB/s \n",
            "\u001b[?25hCollecting prefixed>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/54/70/5356a73361214257618f2ff07afb539a44f4519db4c3112b981f910e02a1/prefixed-0.3.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.7/dist-packages (from blessed>=1.17.7->enlighten) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from blessed>=1.17.7->enlighten) (1.15.0)\n",
            "Building wheels for collected packages: pytrie, ipapy\n",
            "  Building wheel for pytrie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytrie: filename=PyTrie-0.4.0-cp37-none-any.whl size=6089 sha256=8cae306a9e3c736823bce725ca29d7e3d8c626dc09b6cac27e52a855de533004\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/2a/41/5870cad27097f3b3d7b3d96aa5897d502db08cafba9051bd62\n",
            "  Building wheel for ipapy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipapy: filename=ipapy-0.0.9.0-cp37-none-any.whl size=38724 sha256=737ba5d70b9ad117f16b7dcab773ef1eede7168c7eaaab0e618c783dadb0d90b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/f3/00/9f83f4dada00246acda572d4248dc8215c5a8ca37bb4f173a8\n",
            "Successfully built pytrie ipapy\n",
            "Installing collected packages: pytrie, blessed, prefixed, enlighten, colorlog, inflection, ipapy\n",
            "Successfully installed blessed-1.18.0 colorlog-5.0.1 enlighten-1.10.1 inflection-0.5.1 ipapy-0.0.9.0 prefixed-0.3.2 pytrie-0.4.0\n",
            "Cloning into 'xib'...\n",
            "remote: Enumerating objects: 3093, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 3093 (delta 32), reused 47 (delta 24), pack-reused 3032\u001b[K\n",
            "Receiving objects: 100% (3093/3093), 636.08 KiB | 1.16 MiB/s, done.\n",
            "Resolving deltas: 100% (2434/2434), done.\n",
            "Cloning into 'dev_misc'...\n",
            "remote: Enumerating objects: 2004, done.\u001b[K\n",
            "remote: Counting objects: 100% (437/437), done.\u001b[K\n",
            "remote: Compressing objects: 100% (241/241), done.\u001b[K\n",
            "remote: Total 2004 (delta 311), reused 312 (delta 196), pack-reused 1567\u001b[K\n",
            "Receiving objects: 100% (2004/2004), 344.09 KiB | 2.75 MiB/s, done.\n",
            "Resolving deltas: 100% (1395/1395), done.\n",
            "Note: checking out 'b44fde842a6311e03f731cd4e110dcd9fc394db7'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at b44fde8 py.typed; add_condition for arglib\n",
            "Obtaining file:///content/xib/dev_misc\n",
            "Installing collected packages: dev-misc\n",
            "  Running setup.py develop for dev-misc\n",
            "Successfully installed dev-misc\n",
            "Obtaining file:///content/xib\n",
            "Installing collected packages: xib\n",
            "  Running setup.py develop for xib\n",
            "Successfully installed xib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOvomLGWQdA1"
      },
      "source": [
        "# Setup Dataset\n",
        "\n",
        "There are 2 ways to setup your dataset:\n",
        "1. Mount Gdrive to Colab environment. If you decided to go with this and would like to access the data directly in our `LCT Project` shared folder, you can follow instruction in https://stackoverflow.com/questions/54351852/accessing-shared-with-me-with-colab to load `Preprocessed file` folder in Shared google drive. Essentially you just have to go to the location of the folder, right click and choose `Add a shortcut to Drive`. After that you just have to run the cell below\n",
        "2. Upload your dataset to `sample_data` folder in google colab environment. Be aware that you **will** lose your data in this folder when you restart colab's environment\n",
        "\n",
        "Please do remember that the word **must** be in an alphabetical or IPA form, that means no number, no non-alphabetical characters, etc. Otherwise, it will throw an error. If you're unsure whether your data is correct or not, run the last cell before **Run training** section to check whethere all your vocabs are valid\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RKp9Ccp3sWO"
      },
      "source": [
        "from ipapy.ipastring import IPAString\n",
        "from ipapy import is_valid_ipa\n",
        "\n",
        "\n",
        "dia2char = {\n",
        "    'low': {'à': 'a', 'è': 'e', 'ò': 'o', 'ì': 'i', 'ù': 'u', 'ѐ': 'e', 'ǹ': 'n', 'ỳ': 'y'},\n",
        "    'high': {'á': 'a', 'é': 'e', 'ó': 'o', 'ú': 'u', 'ý': 'y', 'í': 'i', 'ḿ': 'm', 'ĺ': 'l',\n",
        "             'ǿ': 'ø', 'ɔ́': 'ɔ', 'ɛ́': 'ɛ', 'ǽ': 'æ', 'ə́': 'ə', 'ŕ': 'r', 'ń': 'n'},\n",
        "    'rising_falling': {'ã': 'a'},\n",
        "    'falling': {'â': 'a', 'î': 'i', 'ê': 'e', 'û': 'u', 'ô': 'o', 'ŷ': 'y', 'ĵ': 'j'},\n",
        "    'rising': {'ǎ': 'a', 'ǐ': 'i', 'ǔ': 'u', 'ǒ': 'o', 'ě': 'e'},\n",
        "    'extra_short': {'ă': 'a', 'ĕ': 'e', 'ĭ': 'i', 'ŏ': 'o', 'ŭ': 'u'},\n",
        "    'nasalized': {'ĩ': 'i', 'ũ': 'u', 'ã': 'a', 'õ': 'o', 'ẽ': 'e', 'ṽ': 'v', 'ỹ': 'y'},\n",
        "    'breathy_voiced': {'ṳ': 'u'},\n",
        "    'creaky_voiced': {'a̰': 'a', 'ḭ': 'i', 'ḛ': 'e', 'ṵ': 'u'},\n",
        "    'centralized': {'ë': 'e', 'ä': 'a', 'ï': 'i', 'ö': 'o', 'ü': 'u', 'ÿ': 'y'},\n",
        "    'mid': {'ǣ': 'æ', 'ū': 'u', 'ī': 'i', 'ē': 'e', 'ā': 'a', 'ō': 'o'},\n",
        "    'voiceless': {'ḁ': 'a'},\n",
        "    'extra_high': {'ő': 'o'},\n",
        "    'extra_low': {'ȁ': 'a'},\n",
        "    'syllabic': {'ạ': 'a', 'ụ': 'u'}\n",
        "}\n",
        "\n",
        "\n",
        "dia2code = {\n",
        "    'low': 768,\n",
        "    'high': 769,\n",
        "    'rising_falling': 771,\n",
        "    'falling': 770,\n",
        "    'rising': 780,\n",
        "    'extra_short': 774,\n",
        "    'nasalized': 771,\n",
        "    'breathy_voiced': 804,\n",
        "    'creaky_voiced': 816,\n",
        "    'centralized': 776,\n",
        "    'mid': 772,\n",
        "    'voiceless': 805,\n",
        "    'extra_high': 779,\n",
        "    'extra_low': 783,\n",
        "    'syllabic': 809,\n",
        "    'high_rising': 7620,\n",
        "    'low_rising': 7621,\n",
        "}\n",
        "\n",
        "\n",
        "char2ipa_char = dict()\n",
        "for dia, char_map in dia2char.items():\n",
        "    code = dia2code[dia]\n",
        "    s = chr(code)\n",
        "    for one_char, vowel in char_map.items():\n",
        "        char2ipa_char[one_char] = vowel + s\n",
        "\n",
        "\n",
        "to_remove = {'ᶢ', '̍', '-', 'ⁿ', 'ᵑ', 'ᵐ', 'ᶬ', ',', 'ᵊ', 'ˢ', '~', '͍', 'ˣ', 'ᵝ', '⁓', '˭', 'ᵈ', '⁽', '⁾', '˔', 'ᵇ',\n",
        "             '+', '⁻'}\n",
        "\n",
        "\n",
        "def clean(s):\n",
        "    if s == '◌̃':\n",
        "        return ''\n",
        "    return ''.join(c for c in s if c not in to_remove)\n",
        "\n",
        "\n",
        "def sub(s):\n",
        "    return ''.join(char2ipa_char.get(c, c) for c in s)\n",
        "\n",
        "\n",
        "to_standardize = {\n",
        "    'ˁ': 'ˤ',\n",
        "    \"'\": 'ˈ',\n",
        "    '?': 'ʔ',\n",
        "    'ṭ': 'ʈ',\n",
        "    'ḍ': 'ɖ',\n",
        "    'ṇ': 'ɳ',\n",
        "    'ṣ': 'ʂ',\n",
        "    'ḷ': 'ɭ',\n",
        "    ':': 'ː',\n",
        "    'ˇ': '̌',\n",
        "    'ỵ': 'y˞',\n",
        "    'ọ': 'o˞',\n",
        "    'ř': 'r̝',  # Czech\n",
        "    '͈': 'ː',  # Irish\n",
        "    'ŕ̩': sub('ŕ') + '̩',  # sanskrit\n",
        "    'δ': 'd',  # Greek\n",
        "    'ń̩': sub('ń') + '̩',  # unsure\n",
        "    'ε': 'e',\n",
        "    'X': 'x',\n",
        "    'ṍ': sub('õ') + chr(769),\n",
        "    'ÿ̀': sub('ÿ') + chr(768),\n",
        "    '∅': 'ʏ'  # Norvegian,\n",
        "}\n",
        "\n",
        "\n",
        "def get_string(s: str) -> IPAString:\n",
        "    return IPAString(unicode_string=clean(sub(standardize(s))))\n",
        "\n",
        "\n",
        "def standardize(s):\n",
        "    return ''.join(to_standardize.get(c, c) for c in s)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTPLvdDVSfq2"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54OeAkgGUazy"
      },
      "source": [
        "# Set your dataset path here\n",
        "KNOWN_LANG_PATH = \"/content/sample_data/it_testing.txt\"\n",
        "UNKNOWN_LANG_PATH = \"/content/sample_data/srb_lat.txt\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sslhYdK94qXy",
        "outputId": "64e7463b-33b9-4cad-9be0-4e68fdbd9063"
      },
      "source": [
        "print(\"Check KNOWN Language file format\")\n",
        "with open(KNOWN_LANG_PATH, \"r\", encoding=\"utf8\") as f:\n",
        "    for line in f.readlines():\n",
        "        s = clean(sub(standardize(line.strip())))\n",
        "        # print(s, is_valid_ipa(s))\n",
        "        if not is_valid_ipa(s):\n",
        "            print(s, \"is invalid\")\n",
        "\n",
        "\n",
        "print(\"\\nCheck UNKNOWN_LANG_PATH Language file format\")\n",
        "with open(UNKNOWN_LANG_PATH, \"r\", encoding=\"utf8\") as f:\n",
        "    for line in f.readlines():\n",
        "        s = clean(sub(standardize(line.strip())))\n",
        "        # print(s, is_valid_ipa(s))\n",
        "        if not is_valid_ipa(s):\n",
        "            print(s, \"is invalid\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Check KNOWN Language file format\n",
            "dona39signor is invalid\n",
            "acquietata8928ritorn is invalid\n",
            "accetta8929entr is invalid\n",
            "servi8930entr is invalid\n",
            "\n",
            "Check UNKNOWN_LANG_PATH Language file format\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIUTbLWGS04a"
      },
      "source": [
        "# Run Training\n",
        "\n",
        "As of now, the training will run indefinitely and not sure if changing this will affect the rest of the code. For that reason, we need to stop the training manually once we feel that the losses are no longer improving. We can monitor the `ll` variable from script output inside a table with the following format to know when to stop the training (i.e. the `ll` is close to zero)\n",
        "\n",
        "```\n",
        "+----------------------------------------+  \n",
        "|                  3_8                   |  \n",
        "+-----------+----------+--------+--------+  \n",
        "| name      | value    | weight | mean   |  \n",
        "+-----------+----------+--------+--------+  \n",
        "| grad_norm | 58.492   | 60     | 0.975  |  \n",
        "| ll        | -336.246 | 60     | -5.604 |  \n",
        "| reg       | 0.555    | 60     | 0.009  |  \n",
        "+-----------+----------+--------+--------+\n",
        "```\n",
        "\n",
        "The other way to know when to stop the training is to also monitor the output of the model validation. Go to the next section to see how to analyze the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_MjVoIlX1Gf"
      },
      "source": [
        "# Total number of phonetic feature groups\n",
        "NUM_FEATURE_GROUPS = 10\n",
        "\n",
        "# Total number of phonetic features\n",
        "NUM_FEATURES = 10\n",
        "\n",
        "# Initial value of threshold to determine whether two words are matched. This will determine\n",
        "# whether two words are in match. The bigger the value, the more false positive we will have.\n",
        "# However, if the value is too low, the model will not output anything\n",
        "THRESHOLD = 1.5\n",
        "\n",
        "# Cost in doing insertion and deletion operation in edit distance algorithm, refer to the paper for more details\n",
        "INS_DEL_COST = 100.0\n",
        "\n",
        "# Learning rate for adam optimizer\n",
        "LR = 0.002\n",
        "\n",
        "# How many training steps to do before running the evaluation steps\n",
        "EVAL_INTERVAL = 500"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLKgxArhd80V",
        "outputId": "f1334b6d-66e4-4063-aa66-26397f07668c"
      },
      "source": [
        "!PYTHONPATH=/content/xib && /usr/local/bin/python -m xib.main --task extract \\\n",
        "  --vocab_path {KNOWN_LANG_PATH} --data_path {UNKNOWN_LANG_PATH} \\\n",
        "  --dim 112 --min_word_length 1 --max_word_length 10 --input_format text \\\n",
        "  --dense_input --eval_interval 20 \\\n",
        "  --char_per_batch 128 --gpus 0 \\\n",
        "  --num_feature_groups {NUM_FEATURE_GROUPS} --num_features {NUM_FEATURES} \\\n",
        "  --init_threshold {THRESHOLD} --init_ins_del_cost {INS_DEL_COST} --learning_rate {LR}"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/content/xib/dev_misc/dev_misc/utils.py:36: DeprecationWarning: Class/function Trainer deprecated.\n",
            "  warnings.warn(message, warning_cls)\n",
            "/content/xib/dev_misc/dev_misc/utils.py:36: RuntimeWarning: Class/function DecipherEsNoisyItalianP5Test is buggy.\n",
            "  warnings.warn(message, warning_cls)\n",
            "/usr/local/lib/python3.7/dist-packages/ipapy/ipastring.py:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import MutableSequence\n",
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n",
            "\u001b[32mINFO - 05/31/21 14:50:50 - 0:00:00 at parser.py:152 - xib.ipa.process:\n",
            "                                                      \tmin_word_length: 1\n",
            "                                                      xib.data_loader.BaseIpaDataLoader:\n",
            "                                                      \tchar_per_batch: 128\n",
            "                                                      \tdata_path: /content/sample_data/srb_lat.txt\n",
            "                                                      \tnew_style: False\n",
            "                                                      \tnum_workers: 0\n",
            "                                                      xib.data_loader:\n",
            "                                                      \tbroken_words: False\n",
            "                                                      \tinput_format: text\n",
            "                                                      \tmax_segment_length: 10\n",
            "                                                      xib.model:\n",
            "                                                      \tdim: 112\n",
            "                                                      \thidden_size: 5\n",
            "                                                      \tnum_feature_groups: 10\n",
            "                                                      \tnum_features: 10\n",
            "                                                      xib.model.modules.Encoder:\n",
            "                                                      \tdense_input: True\n",
            "                                                      \twindow_size: 3\n",
            "                                                      xib.search.searcher.BeamSearcher:\n",
            "                                                      \tbeam_size: 200\n",
            "                                                      xib.model.lm_model.LM:\n",
            "                                                      \tuse_cbow_encoder: True\n",
            "                                                      \tweighted_loss: \n",
            "                                                      xib.model.lm_model.AdaptLM:\n",
            "                                                      \tprior_value: 0.5\n",
            "                                                      \tuse_moe: False\n",
            "                                                      \tuse_prior: False\n",
            "                                                      xib.model.decipher_model.DecipherModel:\n",
            "                                                      \tadapt_mode: none\n",
            "                                                      \tdropout: 0.0\n",
            "                                                      \tlm_model_path: None\n",
            "                                                      \tn_times: 5\n",
            "                                                      \tnum_heads: 4\n",
            "                                                      \tnum_samples: 100\n",
            "                                                      \tnum_self_attn_layers: 2\n",
            "                                                      \tsampling_temperature: 1.0\n",
            "                                                      \tuse_brute_force: False\n",
            "                                                      \tvocab_path: /content/sample_data/it_testing.txt\n",
            "                                                      xib.model.extract_model.G2PLayer:\n",
            "                                                      \tg2p_window_size: 3\n",
            "                                                      xib.model.extract_model.ExtractModel:\n",
            "                                                      \tcontext_weight: 0.0\n",
            "                                                      \tdebug: False\n",
            "                                                      \tinit_ins_del_cost: 100.0\n",
            "                                                      \tinit_threshold: 1.5\n",
            "                                                      \tmax_num_words: 3\n",
            "                                                      \tmax_word_length: 10\n",
            "                                                      \tmin_ins_del_cost: 3.5\n",
            "                                                      \tunextracted_prob: 0.01\n",
            "                                                      \tuse_adapt: False\n",
            "                                                      xib.training.evaluator.DecipherEvaluator:\n",
            "                                                      \teval_max_num_samples: 0\n",
            "                                                      xib.training.evaluator.ExtractEvaluator:\n",
            "                                                      \tmatched_threshold: 0.99\n",
            "                                                      xib.training.trainer.BaseTrainer:\n",
            "                                                      \tcheck_interval: 2\n",
            "                                                      \teval_interval: 20\n",
            "                                                      \tlearning_rate: 0.002\n",
            "                                                      \tnum_steps: 10\n",
            "                                                      \tsave_interval: 0\n",
            "                                                      xib.training.trainer.LMTrainer:\n",
            "                                                      \tfeat_groups: pcvdst\n",
            "                                                      xib.training.trainer.DecipherTrainer:\n",
            "                                                      \tconcentration: 0.01\n",
            "                                                      \tmlm_coeff: 0.05\n",
            "                                                      \tscore_per_word: 1.0\n",
            "                                                      \tsupervised: False\n",
            "                                                      \twarmup_updates: 4000\n",
            "                                                      xib.training.trainer:\n",
            "                                                      \taccum_gradients: 1\n",
            "                                                      xib.training.trainer.ExtractTrainer:\n",
            "                                                      \treg_hyper: 1.0\n",
            "                                                      \tsave_alignment: False\n",
            "                                                      xib.training.manager:\n",
            "                                                      \ttask: extract\n",
            "                                                      xib.training.manager.DecipherManager:\n",
            "                                                      \taux_train_data_path: None\n",
            "                                                      \tdev_data_path: None\n",
            "                                                      \tfix_phi: False\n",
            "                                                      \tin_domain_dev_data_path: None\n",
            "                                                      \tsaved_model_path: None\n",
            "                                                      \tsaved_path: None\n",
            "                                                      \ttrain_phi: False\n",
            "                                                      xib.training.manager.ExtractManager:\n",
            "                                                      \tanneal_factor: 0.5\n",
            "                                                      \tmin_threshold: 0.01\n",
            "                                                      \toptim_cls: adam\n",
            "                                                      __main__:\n",
            "                                                      \tcfg: None\n",
            "                                                      \tgpus: (0,)\n",
            "                                                      \tlog_dir: log/2021-05-31/default/14-50-50\n",
            "                                                      \tlog_level: INFO\n",
            "                                                      \tmessage: \n",
            "                                                      \trandom_seed: 1234\u001b[0m\n",
            "\u001b[32mINFO - 05/31/21 14:50:51 - 0:00:01 at data_loader.py:47 - Loaded 2218 segments in total from /content/sample_data/srb_lat.txt.unbroken.text.cache.\u001b[0m\n",
            "\u001b[32mINFO - 05/31/21 14:50:51 - 0:00:01 at data_loader.py:133 - Partitioning the data into batches.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/torch/tensor.py:758: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:934.)\n",
            "  return super(Tensor, self).refine_names(names)\n",
            "\u001b[36mIMP - 05/31/21 14:51:08 - 0:00:18 at logger.py:65 - Unit aligner initialized to 0.\u001b[0m\n",
            "\u001b[1;1H\n",
            "\u001b[K\u001b[1;1H\u001b[1;1H\n",
            "\u001b[K\u001b[1;1H\u001b[1;1H\n",
            "\u001b[K\u001b[1;1H\u001b[1;1H\n",
            "\u001b[K\u001b[1;1H\u001b[1;1H\n",
            "\u001b[Ksave   0%||  0/20 [00:00<?, 0.00/s]\u001b[1;1H\u001b[1;1H\u001b[36mIMP - 05/31/21 14:51:08 - 0:00:18 at logger.py:65 - Setting ins_del_cost to 100.0.\u001b[0m\n",
            "\u001b[36mIMP - 05/31/21 14:51:08 - 0:00:18 at logger.py:65 - Setting threshold to 1.5.\u001b[0m\n",
            "\u001b[32mINFO - 05/31/21 14:51:08 - 0:00:18 at base_trainer.py:87 - Found 41558 trainable parameters.\u001b[0m\n",
            "\u001b[36mIMP - 05/31/21 14:51:09 - 0:00:19 at logger.py:65 - Model saved to log/2021-05-31/default/14-50-50/saved.init.\u001b[0m\n",
            "\u001b[Ktotal_step   0%||  0/10 [00:00<?, 0.00/s]\u001b[1;1H\u001b[1;1H\u001b[32mINFO - 05/31/21 14:51:09 - 0:00:19 at base_trainer.py:87 - Found 41558 trainable parameters.\u001b[0m\n",
            "\u001b[K\u001b[1;1H\u001b[1;1H\n",
            "\u001b[Kcheck 100%|| 2/2 [00:04<00:00, 0.50/s]\u001b[1;1H\u001b[1;1H\u001b[32mINFO - 05/31/21 14:51:12 - 0:00:22 at base_trainer.py:137 - +-----------------------------------------+\n",
            "                                                            |                   0_2                   |\n",
            "                                                            +-----------+----------+--------+---------+\n",
            "                                                            | name      | value    | weight | mean    |\n",
            "                                                            +-----------+----------+--------+---------+\n",
            "                                                            | grad_norm | 228.336  | 32     | 7.136   |\n",
            "                                                            | ll        | -602.741 | 32     | -18.836 |\n",
            "                                                            | reg       | 3.797    | 32     | 0.119   |\n",
            "                                                            +-----------+----------+--------+---------+\u001b[0m\n",
            "\u001b[Kcheck 100%|| 2/2 [00:01<00:00, 2.24/s]\u001b[1;1H\u001b[1;1H\u001b[32mINFO - 05/31/21 14:51:14 - 0:00:24 at base_trainer.py:137 - +---------------------------------------+\n",
            "                                                            |                  0_4                  |\n",
            "                                                            +-----------+----------+--------+-------+\n",
            "                                                            | name      | value    | weight | mean  |\n",
            "                                                            +-----------+----------+--------+-------+\n",
            "                                                            | grad_norm | 177.322  | 57     | 3.111 |\n",
            "                                                            | ll        | -427.487 | 57     | -7.5  |\n",
            "                                                            | reg       | 4.678    | 57     | 0.082 |\n",
            "                                                            +-----------+----------+--------+-------+\u001b[0m\n",
            "\u001b[Kcheck 100%|| 2/2 [00:01<00:00, 1.72/s]\u001b[1;1H\u001b[1;1H\u001b[32mINFO - 05/31/21 14:51:16 - 0:00:26 at base_trainer.py:137 - +-----------------------------------------+\n",
            "                                                            |                   0_6                   |\n",
            "                                                            +-----------+----------+--------+---------+\n",
            "                                                            | name      | value    | weight | mean    |\n",
            "                                                            +-----------+----------+--------+---------+\n",
            "                                                            | grad_norm | 172.481  | 43     | 4.011   |\n",
            "                                                            | ll        | -500.455 | 43     | -11.638 |\n",
            "                                                            | reg       | 4.645    | 43     | 0.108   |\n",
            "                                                            +-----------+----------+--------+---------+\u001b[0m\n",
            "\u001b[Kcheck 100%|| 2/2 [00:01<00:00, 2.23/s]\u001b[1;1H\u001b[1;1H\u001b[32mINFO - 05/31/21 14:51:18 - 0:00:28 at base_trainer.py:137 - +-----------------------------------------+\n",
            "                                                            |                   0_8                   |\n",
            "                                                            +-----------+----------+--------+---------+\n",
            "                                                            | name      | value    | weight | mean    |\n",
            "                                                            +-----------+----------+--------+---------+\n",
            "                                                            | grad_norm | 173.846  | 43     | 4.043   |\n",
            "                                                            | ll        | -495.632 | 43     | -11.526 |\n",
            "                                                            | reg       | 4.062    | 43     | 0.094   |\n",
            "                                                            +-----------+----------+--------+---------+\u001b[0m\n",
            "\u001b[Kcheck 100%|| 2/2 [00:01<00:00, 1.72/s]\u001b[1;1H\u001b[1;1H\u001b[32mINFO - 05/31/21 14:51:20 - 0:00:30 at base_trainer.py:137 - +-----------------------------------------+\n",
            "                                                            |                   0_10                  |\n",
            "                                                            +-----------+----------+--------+---------+\n",
            "                                                            | name      | value    | weight | mean    |\n",
            "                                                            +-----------+----------+--------+---------+\n",
            "                                                            | grad_norm | 196.136  | 43     | 4.561   |\n",
            "                                                            | ll        | -486.968 | 43     | -11.325 |\n",
            "                                                            | reg       | 3.669    | 43     | 0.085   |\n",
            "                                                            +-----------+----------+--------+---------+\u001b[0m\n",
            "\u001b[Ktotal_step   0%||  0/10 [00:00<?, 0.00/s]\u001b[1;1H\u001b[1;1H\u001b[32mINFO - 05/31/21 14:51:20 - 0:00:30 at base_trainer.py:87 - Found 41558 trainable parameters.\u001b[0m\n",
            "\u001b[Kcheck 100%|| 2/2 [00:01<00:00, 2.58/s]\u001b[1;1H\u001b[1;1H\u001b[32mINFO - 05/31/21 14:51:22 - 0:00:32 at base_trainer.py:137 - +----------------------------------------+\n",
            "                                                            |                  1_2                   |\n",
            "                                                            +-----------+----------+--------+--------+\n",
            "                                                            | name      | value    | weight | mean   |\n",
            "                                                            +-----------+----------+--------+--------+\n",
            "                                                            | grad_norm | 129.64   | 50     | 2.593  |\n",
            "                                                            | ll        | -452.366 | 50     | -9.047 |\n",
            "                                                            | reg       | 3.588    | 50     | 0.072  |\n",
            "                                                            +-----------+----------+--------+--------+\u001b[0m\n",
            "\u001b[Kcheck 100%|| 2/2 [00:01<00:00, 1.93/s]\u001b[1;1H\u001b[1;1H\u001b[32mINFO - 05/31/21 14:51:24 - 0:00:33 at base_trainer.py:137 - +----------------------------------------+\n",
            "                                                            |                  1_4                   |\n",
            "                                                            +-----------+----------+--------+--------+\n",
            "                                                            | name      | value    | weight | mean   |\n",
            "                                                            +-----------+----------+--------+--------+\n",
            "                                                            | grad_norm | 154.012  | 63     | 2.445  |\n",
            "                                                            | ll        | -361.799 | 63     | -5.743 |\n",
            "                                                            | reg       | 3.657    | 63     | 0.058  |\n",
            "                                                            +-----------+----------+--------+--------+\u001b[0m\n",
            "\u001b[Kcheck 100%|| 2/2 [00:01<00:00, 1.51/s]\u001b[1;1H\u001b[1;1H\u001b[32mINFO - 05/31/21 14:51:26 - 0:00:36 at base_trainer.py:137 - +----------------------------------------+\n",
            "                                                            |                  1_6                   |\n",
            "                                                            +-----------+---------+--------+---------+\n",
            "                                                            | name      | value   | weight | mean    |\n",
            "                                                            +-----------+---------+--------+---------+\n",
            "                                                            | grad_norm | 141.578 | 41     | 3.453   |\n",
            "                                                            | ll        | -481.23 | 41     | -11.737 |\n",
            "                                                            | reg       | 3.713   | 41     | 0.091   |\n",
            "                                                            +-----------+---------+--------+---------+\u001b[0m\n",
            "\u001b[Kcheck 100%|| 2/2 [00:01<00:00, 2.59/s]\u001b[1;1H\u001b[1;1H\u001b[32mINFO - 05/31/21 14:51:27 - 0:00:37 at base_trainer.py:137 - +----------------------------------------+\n",
            "                                                            |                  1_8                   |\n",
            "                                                            +-----------+----------+--------+--------+\n",
            "                                                            | name      | value    | weight | mean   |\n",
            "                                                            +-----------+----------+--------+--------+\n",
            "                                                            | grad_norm | 113.501  | 74     | 1.534  |\n",
            "                                                            | ll        | -309.017 | 74     | -4.176 |\n",
            "                                                            | reg       | 3.771    | 74     | 0.051  |\n",
            "                                                            +-----------+----------+--------+--------+\u001b[0m\n",
            "\u001b[Kcheck 100%|| 2/2 [00:01<00:00, 1.92/s]\u001b[1;1H\u001b[1;1H\u001b[32mINFO - 05/31/21 14:51:29 - 0:00:39 at base_trainer.py:137 - +-----------------------------------------+\n",
            "                                                            |                   1_10                  |\n",
            "                                                            +-----------+----------+--------+---------+\n",
            "                                                            | name      | value    | weight | mean    |\n",
            "                                                            +-----------+----------+--------+---------+\n",
            "                                                            | grad_norm | 89.037   | 42     | 2.12    |\n",
            "                                                            | ll        | -451.117 | 42     | -10.741 |\n",
            "                                                            | reg       | 3.79     | 42     | 0.09    |\n",
            "                                                            +-----------+----------+--------+---------+\u001b[0m\n",
            "\u001b[Keval_batch   7%||  6/89 [00:01<00:11, 7.74/s]\u001b[1;1H\u001b[1;1HTraceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/xib/xib/main.py\", line 51, in <module>\n",
            "    train()\n",
            "  File \"/content/xib/xib/main.py\", line 38, in train\n",
            "    manager.run()\n",
            "  File \"/content/xib/xib/training/manager.py\", line 266, in run\n",
            "    self.trainer.train(self.dl_reg)\n",
            "  File \"/content/xib/dev_misc/dev_misc/trainlib/base_trainer.py\", line 104, in train\n",
            "    eval_metrics = self.try_evaluate()\n",
            "  File \"/content/xib/dev_misc/dev_misc/trainlib/base_trainer.py\", line 150, in try_evaluate\n",
            "    return self.evaluate()\n",
            "  File \"/content/xib/dev_misc/dev_misc/trainlib/base_trainer.py\", line 153, in evaluate\n",
            "    eval_metrics = self.evaluator.evaluate(self.stage)\n",
            "  File \"/content/xib/xib/training/evaluator.py\", line 270, in evaluate\n",
            "    ret = self.model(batch)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/xib/xib/model/extract_model.py\", line 273, in forward\n",
            "    new_extracted = self._extract_one_span(batch, extracted, word_repr, unit_repr, char_log_probs)\n",
            "  File \"/content/xib/xib/model/extract_model.py\", line 354, in _extract_one_span\n",
            "    matches = self._get_matches(extracted_word_repr, unit_repr, viable_lens, extracted_unit_ids, char_log_probs)\n",
            "  File \"/content/xib/xib/model/extract_model.py\", line 438, in _get_matches\n",
            "    viable_i = get_range(ns, 2, 0)\n",
            "  File \"/content/xib/dev_misc/dev_misc/devlib/helper.py\", line 58, in get_range\n",
            "    return get_tensor(torch.arange(size).long().reshape(*shape), cpu=cpu)\n",
            "  File \"/content/xib/dev_misc/dev_misc/devlib/helper.py\", line 39, in get_tensor\n",
            "    tensor = tensor.cuda()\n",
            "KeyboardInterrupt\n",
            "\u001b[1;0r\u001b[1;1H\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aS6jXpiZ7Ua"
      },
      "source": [
        "# Analyzing The Result\n",
        "\n",
        "The result will be in the following file `log/<DATE>/default/<TIME>/predictions/extract.<EPOCH>_<STEPS>.tsv`\n",
        "\n",
        "For some reason, google colab won't show anything under `log` folder, so I would suggest to analyze it via `cat` command or download the result to your local computer and analyze it from there.\n",
        "\n",
        "The content inside of the `tsv` file will consist of 4 different columns: `segment`, `ground_truth`, `prediction`, `matched_segment`. From my understanding, `segment` is the original segment of the unknown language; `ground_truth` similar to `segment` but with their exact index locations; `prediction` is the vocabulary prediction in the known language; and `matched_segment` is the information on which segment the unknown language match the vocabulary in known language. If you want, you can get more details by looking at the code in `evaluator.py` line 257\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubz4eYmdf7MM"
      },
      "source": [
        "# You can run the following command **sequentially**\n",
        "# !ls log/ # to get the list of experiment dates\n",
        "# !ls log/<date>/default/ # to get the list of experiment times, first replace date from the command above\n",
        "# !ls log/<date>/default/<time>/predictions/<filename> # to get the list of filename, first replace date and time from the commands above\n",
        "\n",
        "# Replace <date>, <time>, and <filename> from the commands above\n",
        "# !cat log/<date>/default/<time>/predictions/<filename>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPuUbwNwdZ0Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1aea28a0-590c-463a-aba7-b3fdbdcced37"
      },
      "source": [
        "# Or you can run the following command to download the result to your local environment\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"log/2021-05-29/default/09-15-25/predictions/extract.1_10.tsv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1c071154-88f2-493b-90dc-afb1adb3ce53\", \"extract.1_10.tsv\", 314792)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4kdXzy2aFZP"
      },
      "source": [
        "The other way around is to direct the output to your google drive folder by setting up `--log_dir` parameter on the script. However, by setting up this parameter we won't have the same directory structure format as if we don't set the parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEOIhpbdd2DG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}